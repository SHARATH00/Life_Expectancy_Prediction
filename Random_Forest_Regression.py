# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LGW8iig9q0Fvy6PTdf11mQ2M8TuJmuHR
"""

# Commented out IPython magic to ensure Python compatibility.
#Libraries
# %matplotlib inline
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#import sklearn
#!pip install sklearn.impute
#!pip install -U scikit-learn
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder

#from sklearn.impute import Imputer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

pd.set_option("display.max_columns", 101)

"""# **Data Wrangling & Visualization**"""

# Dataset is already loaded below
df = pd.read_csv("train.csv",index_col=0)
# Explore the first five rows
df.head()

# Find number of rows and columns
df.shape
# Explore column types
df.dtypes
#Description
df.describe()

#Discriptive statistic indicates that in general "surface_area" values (total area of lands) 
#are smaller than "argicultural_land" and "forst_area" values (argicultural and forest portions of the total area). It seems there is data entry problem. 
#Following analysese are preformed to address this issue.
# Check if these three variables are highly correlated (multicollinearity problem)
sns.heatmap(df[['surface_area', 'agricultural_land','forest_area']].corr(),
            cmap='PRGn',annot=True);

# Plot pairwise relationship and distribution of these three variables
sns.pairplot(df[['surface_area', 'agricultural_land','forest_area']]);

"""***There are strong linear relashionships among 'surface_area', 'agricultural_land', and 'forest_area'.*** Therefore, having these three variables altogether will not help (provide additional information) future prediction. To resolve this problem the following steps are taken:

*   Multiply 'surface_area' by 100 (to fix data entry/unit issue).
*   Create two new features (colums) namely 'agricultural_portion' and 'forest_portion' by dividing 'agricultural_land' and 'forest_area' by 'surface_area' respectively.
*   Remove 'agricultural_land' and 'forest_area' colums from dataset.
"""

#*There are strong linear relashionships among 'surface_area', 'agricultural_land', and 'forest_area'.* Therefore, having these three variables altogether will not help (provide additional information) future prediction. To resolve this problem the following steps are taken:

#Multiply 'surface_area' by 100 (to fix data entry/unit issue).
#Create two new features (colums) namely 'agricultural_portion' and 'forest_portion' by dividing 'agricultural_land' and 'forest_area' by 'surface_area' respectively.
#Remove 'agricultural_land' and 'forest_area' colums from dataset.

# make a copy of the orginal dataframe
df1 = df.copy()

# Step 1: Multiply 'surface_area' by 100
df1['surface_area'] = df1['surface_area'] * 100

# Step 2: Creates new variables 
df1['agricultural_portion'] = df1['agricultural_land'] / df1['surface_area'] 
df1['forest_portion'] = df1['forest_area'] / df1['surface_area']

# Step 3: Remove 'agricultural_land' and 'forest_area' colums from dataset
df1.drop(['agricultural_land', 'forest_area'], axis=1, inplace=True)

# Show the first 10 row of df1
df1.head(10)

# Check correlations among 'surface_area', 'agriculture_portion' & 'forest_portion' for multicollinearity
sns.heatmap(df1[['surface_area', 'agricultural_portion','forest_portion']].corr(),
            cmap='PRGn',annot=True);

"""**The variable description indicates 'inflation_annual', 'inflation_monthly', and 'inflarion_weekly' are linearly dependent.**
Therefore, we will keep only 'inflation_annual' column and replace NaN values using information from the other two features (if possible).
"""

#The variable description indicates 'inflation_annual', 'inflation_monthly', and 'inflarion_weekly' are linearly dependent.
#Therefore, we will keep only 'inflation_annual' column and replace NaN values using information from the other two features (if possible).
# Check missing values
df1.isnull().sum()

# To find for how many instances all the three values are missing
#len(df1[(df1['inflation_annual'].isnull()) & (df1['inflation_monthly'].isnull()) & (df1['inflation_weekly'].isnull())])

# Replace missing values in column 'inflation_annual' with information from
# columns 'inflation_monthly' & 'inflation_weekly'

for i in df1.index:
    if np.isnan(df1.loc[i, 'inflation_annual']):
        if np.isnan(df1.loc[i, 'inflation_monthly']):
            df1.loc[i, 'inflation_annual'] = 52 * df1.loc[i, 'inflation_weekly']
        else:
            df1.loc[i, 'inflation_annual'] = 12 * df1.loc[i, 'inflation_monthly']

# Remove columns 'inflation_monthly' & 'inflation_weekly'
df1.drop(['inflation_monthly', 'inflation_weekly'], axis=1, inplace=True)

df1.isnull().sum()

# Convert categorical and ordinal fearures into numeric features
# Decide which categorical variables you want to use in model
for col_name in df1.columns:
    if df1[col_name].dtypes == 'object':
        unique_cat = len(df1[col_name].unique())
        print("Feature '{col_name}' has {unique_cat} unique categories".format(col_name=col_name, unique_cat=unique_cat))

"""**There are too many categories in 'internet_users'. It's more simillar to a numeric feature! Let's check**"""

#There are too many categories in 'internet_users'. It's more simillar to a numeric feature! Let's check...
# Explore values in 'internet_users' column
df1['internet_users']

# Convert 'internet_users' column to associated percentages
variable_split = df1['internet_users'].str.split()
df1['percent_internet_users'] = (pd.to_numeric(variable_split.str.get(0), errors='coerce') / 
                          pd.to_numeric(variable_split.str.get(2), errors='coerce'))
# Remove 'internet_users' column
df1.drop('internet_users', axis=1, inplace=True)

df1.head()

# Check the frequency of categories (labels) in each categorical variable 
for name in df1.select_dtypes(include=['object']):
    print(name,':')
    print(df1[name].value_counts(),'\n')

# Assign "mobile_subscriptions" values to 1 if mobile subscriptions is
# less than 1 per person, otherwise 2:
df1['mobile_subscriptions'] = [1 if x == 'less than 1 per person' else 2 for x in df1['mobile_subscriptions']]

# Assign "women_parliament_seats_rate" to 1 if women _parliament seat _rate is
# [0%-25%), 2 if it is [25%-75%), or 3 if unknown:

df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[0%-25%)', 1))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[25%-50%)', 2))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('[50%-75%)', 2))
df1['women_parliament_seats_rate'] = (
    df1['women_parliament_seats_rate'].replace('unknown', 3))

# Assign numeric values to the levels of "national_income" (ordinal variable):
mapper_1 = {'very low': 1, 'medium low': 2, 'low': 3,
            'medium high': 4, 'high': 5, 'very high': 6,
            'unknown': 7}
df1['national_income'].replace(mapper_1, inplace=True)

# Assign numeric values to the levels of "improved_sanitation" (Ordinal Variable):
mapper_2 = {'very low access': 1, 'low access': 2, 'medium access': 3,
            'high access': 4, 'very high access': 5, 'no info': 6}
df1['improved_sanitation'].replace(mapper_2, inplace=True)

"""Handling Missing Data"""

# make a copy of df1  
df2 = df1.copy()
# How much of your data is missing?
df2.isnull().sum().sort_values(ascending=False)

# Impute missing values using Imputer in sklearn.preprocessing
Imputer = SimpleImputer(missing_values='NaN', strategy='mean')

#from sklearn.impute import SimpleImputer
imp = Imputer(missing_values='NaN', strategy='median', axis=0)
imp.fit(df2)
df2 = pd.DataFrame(data=imp.transform(df2) , columns=df2.columns)

"""Didnt use Imputer to replace Nan with median values """

df2 = df2.fillna(df2.median())

df2.head(10)

"""# **Visualization, Modeling, Machine Learning**
Construct a reliable model that predicts the life expectancy of an area (country, region, group of countries) using socioeconomic variables and identify how different features influence their decision. Explain your findings effectively to technical and non-technical audiences using comments and visualizations, if appropriate.

*   Build an optimized model that effectively solves the business problem.
* 
The model would be evaluated on the basis of Mean Absolute Error. 
* Read the Test.csv file and prepare features for testing.


"""

def plot_histogram(x):
    plt.hist(x, color='gray', edgecolor='black', alpha=0.8)
    plt.title("Histogram of '{var_name}'".format(var_name=x.name))
    plt.xlabel("Value")
    plt.ylabel("Frequency")
    plt.show()

# Plot distribution of traget (outcome) variable in the training data
plot_histogram(df2['life_expectancy'])

#Loading Test data
test_data=pd.read_csv('test.csv',index_col=0)
test_data.head()

# Create a copy of test dataframe
tdf = test_data.copy()

# Step 1: Multiply 'surface_area' by 100
tdf['surface_area'] = tdf['surface_area'] * 100

# Step 2: Creates new variables 
tdf['agricultural_portion'] = tdf['agricultural_land'] / tdf['surface_area'] 
tdf['forest_portion'] = tdf['forest_area'] / tdf['surface_area']

# Step 3: Remove 'agricultural_land' and 'forest_area' colums from dataset
tdf.drop(['agricultural_land', 'forest_area'], axis=1, inplace=True)

# Step 4: Replace missing values in column 'inflation_annual' with information from
# columns 'inflation_monthly' & 'inflation_weekly'

for i in tdf.index:
    if np.isnan(tdf.loc[i, 'inflation_annual']):
        if np.isnan(tdf.loc[i, 'inflation_monthly']):
            tdf.loc[i, 'inflation_annual'] = 52 * tdf.loc[i, 'inflation_weekly']
        else:
            tdf.loc[i, 'inflation_annual'] = 12 * tdf.loc[i, 'inflation_monthly']

# Step5: Remove columns 'inflation_monthly' & 'inflation_weekly'
tdf.drop(['inflation_monthly', 'inflation_weekly'], axis=1, inplace=True)

# Step6: Convert 'internet_users' column to associated percentages
variable_split = tdf['internet_users'].str.split()
tdf['percent_internet_users'] = (pd.to_numeric(variable_split.str.get(0), errors='coerce') / 
                          pd.to_numeric(variable_split.str.get(2), errors='coerce'))

# Step 7: Remove 'internet_users' column
tdf.drop('internet_users', axis=1, inplace=True)

# Step 8: Assign "mobile_subscriptions" values to 1 if mobile subscriptions is
# less than 1 per person, otherwise 2:
tdf['mobile_subscriptions'] = [1 if x == 'less than 1 per person' else 2 for x in tdf['mobile_subscriptions']]

# Step 9: Assign "women_parliament_seats_rate" to 1 if women _parliament seat _rate is
# [0%-25%), 2 if it is [25%-75%), or 3 if unknown:

tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[0%-25%)', 1))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[25%-50%)', 2))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('[50%-75%)', 2))
tdf['women_parliament_seats_rate'] = (
    tdf['women_parliament_seats_rate'].replace('unknown', 3))

# Step 10: Assign numeric values to the levels of "national_income" (ordinal variable):
mapper_1 = {'very low': 1, 'medium low': 2, 'low': 3,
            'medium high': 4, 'high': 5, 'very high': 6,
            'unknown': 7}
tdf['national_income'].replace(mapper_1, inplace=True)

# Step 11: Assign numeric values to the levels of "improved_sanitation" (Ordinal Variable):
mapper_2 = {'very low access': 1, 'low access': 2, 'medium access': 3,
            'high access': 4, 'very high access': 5, 'no info': 6}
tdf['improved_sanitation'].replace(mapper_2, inplace=True)

# Step12: Impute missing values using Imputer in sklearn.preprocessing

#imp = Imputer(missing_values='NaN', strategy='median', axis=0)
#imp.fit(tdf)
#tdf = pd.DataFrame(data=imp.transform(tdf) , columns=tdf.columns)
tdf = tdf.fillna(tdf.median())

# Show the first 5 rows of tdf
tdf.head()

"""# **Random Forest Regression**"""

# Create outcome and input DataFrames
y = df2['life_expectancy'] 
X = df2.drop('life_expectancy', axis=1)
y.head()

# Create train and validation datasets to build the Random Forest (RF) regression 
# model and find the best set of the model parameters 

#X_train, X_validation, y_train, y_validation= train_test_split(X, y,
#                                                   random_state = 0)

# Use 'Grid Search' to find the best set of RF regression parameters 
# using full dataset with criterion = 'squared_error' and 
# random_state = 33


n_estimators = [50, 100, 150, 200]
max_features = [4, 8, 13]
max_depth = [5, 6, 7]
min_split = [2, 3, 4] 
min_leaf = [1, 2, 3]
best_score = 100

for n in n_estimators:
    for f in max_features:
        for d in max_depth:
            for s in min_split:   
                for l in min_leaf:
                     rf = RandomForestRegressor(
                     n_estimators = n, 
                     criterion = 'friedman_mse', 
                     max_features= f,
                     random_state = 33, 
                     oob_score = False,
                     max_depth = d, min_samples_split = s, 
                     min_samples_leaf = l)
            rf.fit(X, y)    
            y_model = rf.predict(X)
            #score = mean_absolute_error(y, y_model)
            score = - np.mean(cross_val_score(rf, X, y, cv=4, scoring = 'neg_mean_absolute_error'))
            if score <= best_score:
                best_score = score
                max_n = n
                max_f = f
                max_d = d
                max_s = s
                max_l = l


print ("Number of Estimators:", max_n)               
print ("Max features:", max_f)
print ("Max Depth:", max_d)
print ("Min Split:", max_s)
print ("Min Leaf:", max_l)
print("Best Mean Absolute Error: {:.3f}".format(best_score))

# Fit a RF using best indentified parameters
rf = RandomForestRegressor(n_estimators=200, criterion = 'friedman_mse', max_features=8, random_state = 33,
                           max_depth=7, min_samples_split=4, min_samples_leaf=3)

rf.fit(X, y)

# Pridict test instances using test dataframe (tdf)
y_test = rf.predict(tdf)
y_test

# Plot histograms to compare distribution of actual outcomes vs. prediction 
def plot_histogram_comp(x,y):
    plt.hist(x, alpha=0.5, edgecolor='black', label='Actual')
    plt.hist(y, alpha=0.5, edgecolor='black', label='Prediction')
    plt.title("Histogram of actual outcomes v.s predicted outcomes")
    plt.xlabel("Value")
    plt.ylabel("Frequency")
    plt.legend(loc='upper left')
    plt.show()

# Check to see if distribution of actual target values is close to the distribution
# of predicted target values
plot_histogram_comp(y,y_test)

"""The government wants to know what are the most important features for your model.

Task:
Visualize the top 13 features and their feature importance.
"""

# Extract feature importance determined by RF model
feature_imp = pd.Series(rf.feature_importances_, index=X.columns)
feature_imp.sort_values(ascending=True, inplace=True)

# Creating a bar plot
feature_imp.plot(kind='barh', width=0.8, figsize=(7,4));

"""Submit the predictions on the test dataset using your optimized model
For each record in the test set (Test.csv), you must predict the value of the life_expectancy variable. 

You should submit a CSV file with a header row and one row per test entry. The file (submissions.csv) should have exactly 2 columns:


id

life_expectancy
"""

# Create a submission_df
d = {'id': test_data.index, 'life_expectancy': y_test}
submission_df = pd.DataFrame(data=d)
submission_df

#Submission
submission_df.to_csv('submissions.csv',index=False)